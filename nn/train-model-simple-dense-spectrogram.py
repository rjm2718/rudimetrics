#!/usr/bin/python3

"""
Creates a tensorflow model and trains it on sample data loaded from .wav/.labels files generated by mktrainingfiles.py.
Input wav file is taken as a stream to be broken up into training examples.  .labels file indicates exact offsets
in stream where examples are, and we can stride through rest of stream for negative examples.

We can experiment with the samples as either/both time domain patterns, or frequency domain spectrograms, fit for
convolutional networks, or a time series recurrent network (but each sample is usually very short, so I'm guessing
convolutional or simple dense networks will work best).


"""
import argparse
import sys
import random

import numpy as np

from dsp import utils as u

# globals
SR = 44100  # sample rate
SPTGRM_FREQ_BINS = 512  # half the window size when creating spectrogram,
SPTGRM_STRIDE = 128    # spectrogram stride, approx
SPTGRM_SAMPLEN = 16384  # spectrogram is created from this length of sample, 370ms in this case

class Sample:
    def __init__(self, data, offset, labels):
        self.data = data
        self.offset = offset
        self.labels = labels
    def __len__(self):
        return len(self.data)
    def __str__(self):
        return '{} @ {}'.format(self.labels, self.offset)

def extract_samples(fn_wav, fn_labels, class_labels):

    def check_sr(sr, data, _):
        if sr != SR: raise Exception('sample rate %d of wave file != %d' % (sr, SR))
        return data
    wd = u.load_wav(fn_wav, check_sr)
    print('loaded wav file, shape=', wd.shape)

    # key is offset, value is Sample
    samples = {}

    # extract labeled samples from wav file, discarding labels not in class_labels
    for l in open(fn_labels):
        (offset, labels) = l.strip().split(':')
        offset = int(offset)
        labels = list(filter(lambda l: l in class_labels, labels.split(',')))
        samples[offset] = Sample(wd[offset:offset+SPTGRM_SAMPLEN], offset, labels)
    ns = len(samples)

    # given offsets of Samples, find points in between (but not too close) to create more (negative) Samples
    offsets = list(samples.keys())
    offsets.sort()
    ns2 = 0
    for i in range(len(offsets)-1):
        o1 = offsets[i] + SPTGRM_SAMPLEN / 4
        o2 = offsets[i+1] - SPTGRM_SAMPLEN / 4
        if (o2-o1) < SPTGRM_SAMPLEN:
            continue
        # we have room for new samples
        n = int((o2-o1)/SPTGRM_SAMPLEN * 2)
        for _ in range(n):
            offset = random.randint(o1, o2)
            samples[offset] = Sample(wd[offset:offset+SPTGRM_SAMPLEN], offset, [])
            ns2 += 1
    print('{} samples plus {} new ones from background space'.format(ns, ns2))

    return samples.values()

def load_data(fn_wav, fn_labels, class_labels, pct_train=0.9):
    """
    Read input wav file, make spectrograms, attach labels.

    :param fn_wav: filename of wav file
    :param fn_labels: filename containing labels, each line offset:labels (labels comma separated)
    :param class_labels: list of labels to use; any other label from input file will be assigned neg/0
    :param pct_train: fraction of input to be used during test
    :return: numpy array of training data X, training labels Y, test data X, test labels Y
    """

    # each element in X_train is shape (121,129) - time,freq

    X_train = []
    Y_train = []
    X_test = []
    Y_test = []

    extract_samples(fn_wav, fn_labels, class_labels)
    sys.exit(0)

    it_neg = int(pct_train * len(fl_neg))
    it_pos = int(pct_train * len(fl_pos))

    # insert pos/neg value, will separate into X/Y arrays later
    def s2x(X, fl, v):
        for l in map(u.load_spectrogram, fl):
            l = np.insert(l, 0, v, axis=0)
            X.append(l)

    s2x(X_train, fl_neg[0:it_neg], 0)
    s2x(X_train, fl_pos[0:it_pos], 1)
    X_train = np.asarray(X_train)
    np.random.shuffle(X_train)
    Y_train = X_train[...,0,0].astype('int32')
    X_train = X_train[...,1:,:].astype('float32')
    #print(X_train.shape)
    #print(Y_train.shape)

    s2x(X_test, fl_neg[it_neg:], 0)
    s2x(X_test, fl_pos[it_pos:], 1)
    X_test = np.asarray(X_test)
    np.random.shuffle(X_test)
    Y_test = X_test[...,0,0].astype('int32')
    X_test = X_test[...,1:,:].astype('float32')

    return X_train, Y_train, X_test, Y_test




def main():
    global SR

    ap = argparse.ArgumentParser()
    ap.add_argument("-r", "--sample-rate", dest="SR", help="sample rate", type=int, default=44100)
    ap.add_argument("-l", "--labels", dest="LABELS", help="comma-sep list of labels to use in classifier", type=str, required=True)
    ap.add_argument("WAVFILE", help="input wav file", type=str)
    ap.add_argument("LABELSFILE", help="input labels file", type=str)

    args = ap.parse_args()
    SR = args.SR
    labels = args.LABELS.split(',')

    X_train, Y_train, X_test, Y_test = load_data(args.WAVFILE, args.LABELSFILE, labels)

    print(X_train.shape)
    print(Y_train.shape)
    print('')
    #print(Y_train)

    u.validate_data(X_train)

    return

    import tensorflow as tf
    from tensorflow.keras.models import Model, load_model, Sequential
    from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, Conv1D, Flatten, BatchNormalization
    from tensorflow.keras.regularizers import l2, l1
    from tensorflow.keras.callbacks import TensorBoard

    model = Sequential()

    model.add(BatchNormalization())

    model.add(Conv1D(filters=64,
                     kernel_size=7,
                     strides=2,
                     activation=tf.nn.relu,
                     #kernel_initializer='uniform',
                     #activity_regularizer=l2(.01),
                     #input_shape=(X_train.shape[1:])
                     ))

    model.add(BatchNormalization())
    model.add(Flatten())

    model.add(Dense(64, activation=tf.nn.relu))
    model.add(BatchNormalization())

    model.add(Dropout(0.4))

    model.add(Dense(16, activation=tf.nn.relu,
                        #kernel_initializer='uniform',
                        #kernel_regularizer=l2(0.01),
                        #activity_regularizer=l2(0.01)
                        ))
    model.add(BatchNormalization())

    model.add(Dropout(0.4))

    model.add(Dense(len(labels), activation=tf.nn.softmax))

    #optimizer = tf.keras.optimzizers.AdamOptimizer()
    optimizer = tf.keras.optimizers.RMSprop(lr=0.01)

    model.compile(optimizer=optimizer,
                  #loss='mse',
                  loss='binary_crossentropy',
                  metrics=['accuracy']) # XXX because of class imbalance, try different metric




    #tbCallBack = TensorBoard(log_dir='./tensorboard', histogram_freq=0, write_graph=True, write_images=True)

    # This builds the model for the first time:
    #model.fit(X_train, Y_train, epochs=2, steps_per_epoch=10, callbacks=[tbCallBack])
    #model.fit(X_train, Y_train, epochs=10, steps_per_epoch=20)
    model.fit(X_train, Y_train, epochs=10)

    #model.summary()
    #all_weights = []
    #for layer in model.layers:
    #   w = layer.get_weights()
    #   all_weights.append(w)
    #all_weights = np.array(all_weights)
    #print(all_weights)


    test_loss, test_acc = model.evaluate(X_test, Y_test)

    print('Test accuracy:', test_acc)

    tf.keras.models.save_model(model, 'train-model-simple-dense-spectrogram.hdf5')



if __name__ == '__main__':
    main()
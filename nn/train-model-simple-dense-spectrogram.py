#!/usr/bin/python3

"""
Creates a tensorflow model and trains it on sample data loaded from .wav/.labels files generated by mktrainingfiles.py.
Input wav file is taken as a stream to be broken up into training examples.  .labels file indicates exact offsets
in stream where examples are, and we can stride through rest of stream for negative examples.

We can experiment with the samples as either/both time domain patterns, or frequency domain spectrograms, fit for
convolutional networks, or a time series recurrent network (but each sample is usually very short, so I'm guessing
convolutional or simple dense networks will work best).


"""
import argparse
import sys
import random

import numpy as np

from dsp import utils as u

# globals
SR = 44100  # sample rate
SPTGRM_WINDOW_SIZE = 512  # results in half this many frequency bins
SPTGRM_STRIDE = 128    # spectrogram stride,  ~ 3ms
SPTGRM_SAMPLEN = 32768  # spectrogram is created from this length of sample, 740ms in this case


class Sample:
    def __init__(self, data, offset, labels):
        """
        :param data: 1-chan audio sample, len SPTGRM_SAMPLEN
        :param offset: original location in wav file (not used)
        :param labels: list of (string) labels
        """
        self.data = data
        self.offset = offset
        self.labels = labels
    def __len__(self):
        return len(self.data)
    def __str__(self):
        return '{} @ {}'.format(self.labels, self.offset)

def extract_samples(fn_wav, fn_labels):

    def check_sr(sr, data, _):
        if sr != SR: raise Exception('sample rate %d of wave file != %d' % (sr, SR))
        return data
    wd = u.load_wav(fn_wav, check_sr)
    print('loaded wav file, shape=', wd.shape)

    # key is offset, value is Sample
    samples = {}

    # extract labeled samples from wav file, discarding labels not in class_labels
    for l in open(fn_labels):
        (offset, labels) = l.strip().split(':')
        offset = int(offset)
        if offset+SPTGRM_SAMPLEN < len(wd):
            samples[offset] = Sample(wd[offset:offset+SPTGRM_SAMPLEN], offset, labels.split(','))
    ns = len(samples)

    # given offsets of Samples, find points in between (but not too close) to create more (negative) Samples
    offsets = list(samples.keys())
    offsets.sort()
    ns2 = 0
    for i in range(len(offsets)-1):
        o1 = offsets[i] + SPTGRM_SAMPLEN / 4
        o2 = offsets[i+1] - SPTGRM_SAMPLEN / 4
        if (o2-o1) < SPTGRM_SAMPLEN:
            continue
        # we have room for new samples
        n = int((o2-o1)/SPTGRM_SAMPLEN * 2)
        for _ in range(n):
            offset = random.randint(o1, o2)
            samples[offset] = Sample(wd[offset:offset+SPTGRM_SAMPLEN], offset, [])
            ns2 += 1
    print('{} samples plus {} new ones from background space'.format(ns, ns2))

    return list(samples.values())


def labels2vec(labels, labels_map):
    """
    # multi-label classifier: labels is a list length N (number of possible labels), each position 1 or 0
    :param labels: list of string labels
    :param labels_map: string to position
    :return: label vector
    """
    v = [0] * len(labels_map)
    for l in labels:
        if l in labels_map:
            i = labels_map[l]
            v[i] = 1
    return v

def vec2labels(lv, labels_map):
    ls = []
    for l,i in labels_map.items():
        if lv[i]:
            ls.append(l)
    return ls

def load_data(fn_wav, fn_labels, labels_map, pct_train=0.9):
    """
    Read input wav file, make spectrograms, attach labels.

    :param fn_wav: filename of wav file
    :param fn_labels: filename containing labels, each line offset:labels (labels comma separated)
    :param labels_map: list of labels to use; any other label from input file will be assigned neg/0
    :param pct_train: fraction of input to be used during test
    :return: numpy array of training data X, training labels Y, test data X, test labels Y
    """

    # each element in X_train is shape (249,257) - time,freq

    train_lcount = {l:0 for l in labels_map.keys()}

    X_train = []
    Y_train = []
    X_test = []
    Y_test = []

    samples = extract_samples(fn_wav, fn_labels)
    random.shuffle(samples)

    samples_train = samples[0:int(pct_train * len(samples))]
    for s in samples_train:
        d = u.spectrogram(s.data, SPTGRM_WINDOW_SIZE, SPTGRM_STRIDE)
        l = labels2vec(s.labels, labels_map)
        X_train.append(d)
        Y_train.append(np.array(l))
        for l in filter(lambda z: z in train_lcount, s.labels):
            train_lcount[l] += 1

    samples_test = samples[int(pct_train * len(samples)):]
    for s in samples_test:
        d = u.spectrogram(s.data, SPTGRM_WINDOW_SIZE, SPTGRM_STRIDE)
        l = labels2vec(s.labels, labels_map)
        X_test.append(d)
        Y_test.append(np.array(l))

    print('training set sample counts:')
    for l,c in train_lcount.items():
        print(l,c)
    print('')

    X_train = np.array(X_train, copy=False)
    Y_train = np.array(Y_train, copy=False)
    X_test = np.array(X_test, copy=False)
    Y_test = np.array(Y_test, copy=False)
    return X_train[..., np.newaxis], Y_train, X_test[..., np.newaxis], Y_test




def main():
    global SR

    ap = argparse.ArgumentParser()
    ap.add_argument("-r", "--sample-rate", dest="SR", help="sample rate", type=int, default=44100)
    ap.add_argument("-l", "--labels", dest="LABELS", help="comma-sep list of labels to use in classifier", type=str, required=True)
    ap.add_argument("WAVFILE", help="input wav file", type=str)
    ap.add_argument("LABELSFILE", help="input labels file", type=str)

    args = ap.parse_args()
    SR = args.SR

    labels_map = {}  # map label (in model) to integer (will be position in multi-label vector of 1s and 0s)
    le = 0
    for l in args.LABELS.split(','):
        labels_map[l] = le
        le += 1

    X_train, Y_train, X_test, Y_test = load_data(args.WAVFILE, args.LABELSFILE, labels_map)

    print('shape of X_train:', X_train.shape)
    print('')

    # for each sample in X_test, put into separate arrays by label
    td_by_label = {}  # label: (x_test, y_test)
    for i in range(0, X_test.shape[0]):
        ll = vec2labels(Y_train[i], labels_map)
        if not ll:
            ll.append('<none>')
        for l in ll:
            x_test, y_test = td_by_label.get(l, ([], []))
            x_test.append(X_test[i])
            y_test.append(Y_test[i])
            td_by_label[l] = (x_test, y_test)
        if len(ll) > 1:
            pass # TODO multi-label
    for l in td_by_label:
        x_test, y_test = td_by_label[l]
        td_by_label[l] = (np.array(x_test), np.array(y_test))




    # TODO batch size

    import tensorflow as tf
    from tensorflow.keras.models import Model, load_model, Sequential
    from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, Conv1D, Conv2D, Flatten, BatchNormalization
    from tensorflow.keras.regularizers import l2, l1
    from tensorflow.keras.callbacks import TensorBoard

    print('')
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)

        except RuntimeError as e:
            print(e)

    model = Sequential()
    model.blah = 'ok'

    model.add(BatchNormalization(fused=False))

    # model.add(Conv2D(filters=128,
    #                  kernel_size=7,
    #                  strides=2,
    #                  activation=tf.nn.relu,
    #                  #kernel_initializer='uniform',
    #                  #activity_regularizer=l2(.01),
    #                  input_shape=(X_train.shape[1:])
    #                  ))
    model.add(Conv1D(filters=64,
                     kernel_size=7,
                     strides=2,
                     activation=tf.nn.relu,
                     # kernel_initializer='uniform',
                     # activity_regularizer=l2(.01),
                     input_shape=(X_train.shape[1:])
                     ))

    model.add(BatchNormalization(fused=False))
    model.add(Flatten())

    model.add(Dense(64, activation=tf.nn.relu))
    model.add(BatchNormalization(fused=False))

    model.add(Dropout(0.2))

    model.add(Dense(16, activation=tf.nn.relu,
                        #kernel_initializer='uniform',
                        #kernel_regularizer=l2(0.01),
                        #activity_regularizer=l2(0.01)
                        ))
    model.add(BatchNormalization(fused=False))

    model.add(Dropout(0.2))

    model.add(Dense(len(labels_map), activation=tf.nn.softmax)) # TODO multi-label

    #optimizer = tf.keras.optimzizers.AdamOptimizer()
    optimizer = tf.keras.optimizers.RMSprop(lr=0.01)

    model.compile(optimizer=optimizer,
                  #loss='mse',
                  loss='binary_crossentropy',
                  metrics=['accuracy']) # XXX because of class imbalance, try different metric

    print('')

    #tbCallBack = TensorBoard(log_dir='./tensorboard', histogram_freq=0, write_graph=True, write_images=True)

    # This builds the model for the first time:
    #model.fit(X_train, Y_train, epochs=2, steps_per_epoch=10, callbacks=[tbCallBack])
    #model.fit(X_train, Y_train, epochs=10, steps_per_epoch=20)
    model.fit(X_train, Y_train, epochs=20)

    model.summary()

    #all_weights = []
    #for layer in model.layers:
    #   w = layer.get_weights()
    #   all_weights.append(w)
    #all_weights = np.array(all_weights)
    #print(all_weights)

    print('\nevaluation by label:')
    for l in td_by_label:
        x_test, y_test = td_by_label[l]
        test_loss, test_acc = model.evaluate(x_test, y_test)
        print('%10s:  %.3f' % (l, test_acc))

    tf.keras.models.save_model(model, 't1.hdf5')



if __name__ == '__main__':
    main()

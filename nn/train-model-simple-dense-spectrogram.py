#!/usr/bin/python3

"""
Creates a tensorflow model and trains it on sample data loaded from .wav/.labels files generated by mktrainingfiles.py.
Input wav file is taken as a stream to be broken up into training examples.  .labels file indicates exact offsets
in stream where examples are, and we can stride through rest of stream for negative examples.

We can experiment with the samples as either/both time domain patterns, or frequency domain spectrograms, fit for
convolutional networks, or a time series recurrent network (but each sample is usually very short, so I'm guessing
convolutional or simple dense networks will work best).


"""
import os
import sys, argparse, random, json
import time

import numpy as np

from dsp import utils as u

# globals
BATCH_SIZE = 128
TRAIN_EPOCHS = 40
SR = 44100  # sample rate
SPTGRM_WINDOW_SIZE = 512  # results in half this many frequency bins
SPTGRM_STRIDE = 128    # spectrogram stride,  ~ 3ms
SPTGRM_SAMPLEN = 32768  # spectrogram is created from this length of sample, 740ms in this case


class Sample:
    def __init__(self, data, offset, labels):
        """
        :param data: 1-chan audio sample, len SPTGRM_SAMPLEN
        :param offset: original location in wav file (not used)
        :param labels: list of (string) labels
        """
        self.data = data
        self.offset = offset
        self.labels = labels

    def getFeatureVector(self, labels_map):
        """Use labels_map to create vector of 1s corresponding to the labels.  If there is no mapping,
        then lookup None label in map (which will be position 0 in vector) to set.
        """
        pos_ex = False
        v = [0] * len(labels_map)
        for l in self.labels:
            if l in labels_map:
                i = labels_map[l]
                v[i] = 1
                pos_ex = True

        if not pos_ex:
            v[0] = 1  # negative example, null class

        return v

    def getLabels(self, labels_map=None):
        """return list of labels, the ones set with this object, or filter through labels_map.  If there is
        no label to return (neg example), returns [None]"""
        ll = self.labels
        if labels_map:
            ll = list(filter(lambda l:l in labels_map, self.labels))
        if ll:
            return ll
        return [None]

    def __len__(self):
        return len(self.data)
    def __str__(self):
        return '{} @ {}'.format(self.labels, self.offset)

def extract_samples(fn_wav, fn_labels):

    def check_sr(sr, data, _):
        if sr != SR: raise Exception('sample rate %d of wave file != %d' % (sr, SR))
        # do a conversion?
        return data
    wd = u.load_wav(fn_wav, check_sr)
    print('loaded wav file, shape=', wd.shape)

    # key is offset, value is Sample
    samples = {}

    # extract labeled samples from wav file, discarding labels not in class_labels
    for l in open(fn_labels):
        (offset, labels) = l.strip().split(':')
        offset = int(offset)
        if offset+SPTGRM_SAMPLEN < len(wd):
            samples[offset] = Sample(wd[offset:offset+SPTGRM_SAMPLEN], offset, labels.split(','))
    ns = len(samples)

    # given offsets of Samples, find points in between (but not too close) to create more (negative) Samples
    offsets = list(samples.keys())
    offsets.sort()
    ns2 = 0
    for i in range(len(offsets)-1):
        o1 = offsets[i] + SPTGRM_SAMPLEN / 4
        o2 = offsets[i+1] - SPTGRM_SAMPLEN / 4
        if (o2-o1) < SPTGRM_SAMPLEN:
            continue
        # we have room for new samples
        n = int((o2-o1)/SPTGRM_SAMPLEN * 2)
        for _ in range(n):
            offset = random.randint(o1, o2)
            samples[offset] = Sample(wd[offset:offset+SPTGRM_SAMPLEN], offset, [])
            ns2 += 1
    print('{} samples plus {} new ones from background space'.format(ns, ns2))

    return list(samples.values())


def vec2labels(lv, labels_map):
    """returns list of string labels.  list may be [None] if vector is for a negative example"""
    ls = []
    for l,i in labels_map.items():
        if lv[i]:
            ls.append(l)
    return ls

def load_data(fn_wav, fn_labels, labels_map, pct_train=0.9):
    """
    Read input wav file, make spectrograms, attach labels.

    :param fn_wav: filename of wav file
    :param fn_labels: filename containing labels, each line offset:labels (labels comma separated)
    :param labels_map: list of labels to use; any other label from input file will be assigned neg/0
    :param pct_train: fraction of input to be used during test
    :return: numpy array of training data X, training labels Y, test data X, test labels Y
    """

    # each element in X_train is shape (249,257) - time,freq

    train_lcount = {l:0 for l in labels_map.keys()}

    X_train = []
    Y_train = []
    X_test = []
    Y_test = []

    samples = extract_samples(fn_wav, fn_labels)
    random.shuffle(samples)

    samples_train = samples[0:int(pct_train * len(samples))]
    for s in samples_train:
        d = u.spectrogram(s.data, SPTGRM_WINDOW_SIZE, SPTGRM_STRIDE)
        y = s.getFeatureVector(labels_map)
        X_train.append(d)
        Y_train.append(np.array(y))

        for l in s.getLabels(labels_map):
            train_lcount[l] += 1

    samples_test = samples[int(pct_train * len(samples)):]
    for s in samples_test:
        d = u.spectrogram(s.data, SPTGRM_WINDOW_SIZE, SPTGRM_STRIDE)
        y = s.getFeatureVector(labels_map)
        X_test.append(d)
        Y_test.append(np.array(y))

    print('training set sample counts:')
    for l,c in train_lcount.items():
        print(l,c)
    print('')

    X_train = np.array(X_train, copy=False)
    Y_train = np.array(Y_train, copy=False)
    X_test = np.array(X_test, copy=False)
    Y_test = np.array(Y_test, copy=False)
    return X_train[..., np.newaxis], Y_train, X_test[..., np.newaxis], Y_test




def main():
    global SR

    ap = argparse.ArgumentParser()
    ap.add_argument("-r", "--sample-rate", dest="SR", help="sample rate", type=int, default=44100)
    ap.add_argument("-l", "--labels", dest="labels", help="comma-sep list of labels to use in classifier", type=str, required=True)
    ap.add_argument("-o", "--model-savefile", dest="modelSaveFile",
                    help="if specified, save trained model in dir with this name, tf SavedModel format, .cfg will be added with metadata", type=str)
    ap.add_argument("inputname", help="input base name (.wav/.labels expected)", type=str)

    args = ap.parse_args()
    SR = args.SR

    t0 = time.time()

    # map label (in model) to integer position in multi-label prediction/label vectors
    labels_map = { None: 0 } # 0 position output is for the null/negative class
    le = 1
    for l in args.labels.split(','):
        labels_map[l] = le
        le += 1
    print(labels_map)

    fn_wav = args.inputname + '.wav'
    fn_lbl = args.inputname + '.labels'

    X_train, Y_train, X_test, Y_test = load_data(fn_wav, fn_lbl, labels_map)

    print('shape of X_train:', X_train.shape)
    print('')

    # for each sample in X_test, put into separate arrays by label
    td_by_label = {}  # label: (x_test, y_test)
    for i in range(0, X_test.shape[0]):
        ll = vec2labels(Y_train[i], labels_map)
        for l in ll:
            x_test, y_test = td_by_label.get(l, ([], []))
            x_test.append(X_test[i])
            y_test.append(Y_test[i])
            td_by_label[l] = (x_test, y_test)
        if len(ll) > 1:
            pass # TODO multi-label
    for l in td_by_label:
        x_test, y_test = td_by_label[l]
        td_by_label[l] = (np.array(x_test), np.array(y_test))

    t1 = time.time()


    # TODO batch size, tf DataSet since things get huge quickly here with the spectrograms

    import tensorflow as tf
    from tensorflow.keras.models import Model, load_model, Sequential
    from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, Conv1D, Conv2D, Flatten, BatchNormalization
    from tensorflow.keras.regularizers import l2, l1
    from tensorflow.keras.callbacks import TensorBoard

    # tf necessary init when using gpu/cuda libs
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            print(e)
            sys.exit(-1)

    print('')
    print('')

    model = Sequential()

    model.add(BatchNormalization(fused=False))

    # model.add(Conv2D(filters=128,
    #                  kernel_size=7,
    #                  strides=2,
    #                  activation=tf.nn.relu,
    #                  #kernel_initializer='uniform',
    #                  #activity_regularizer=l2(.01),
    #                  input_shape=(X_train.shape[1:])
    #                  ))
    model.add(Conv1D(filters=64,
                     kernel_size=7,
                     strides=2,
                     activation=tf.nn.relu,
                     # kernel_initializer='uniform',
                     # activity_regularizer=l2(.01),
                     input_shape=(X_train.shape[1:])
                     ))

    model.add(BatchNormalization(fused=False))
    model.add(Flatten())

    model.add(Dense(64, activation=tf.nn.relu))
    model.add(BatchNormalization(fused=False))

    model.add(Dropout(0.2))

    model.add(Dense(16, activation=tf.nn.relu,
                        #kernel_initializer='uniform',
                        #kernel_regularizer=l2(0.01),
                        #activity_regularizer=l2(0.01)
                        ))
    model.add(BatchNormalization(fused=False))

    model.add(Dropout(0.2))

    model.add(Dense(len(labels_map), activation=tf.nn.sigmoid))

    #optimizer = tf.keras.optimzizers.AdamOptimizer()
    optimizer = tf.keras.optimizers.RMSprop(lr=0.01)

    model.compile(optimizer=optimizer,
                  #loss='mse',
                  loss='binary_crossentropy',
                  metrics=['accuracy']) # XXX because of class imbalance, try different metric

    print('')
    t2 = time.time()


    #tbCallBack = TensorBoard(log_dir='./tensorboard', histogram_freq=0, write_graph=True, write_images=True)

    # This builds the model for the first time
    #model.fit(X_train, Y_train, epochs=2, steps_per_epoch=10, callbacks=[tbCallBack])
    model.fit(X_train, Y_train, epochs=TRAIN_EPOCHS)

    t3 = time.time()
    print('training complete\n')
    model.summary()

    #all_weights = []
    #for layer in model.layers:
    #   w = layer.get_weights()
    #   all_weights.append(w)
    #all_weights = np.array(all_weights)
    #print(all_weights)


    print('\nevaluation by label:')
    recLblAcc = {}
    for l in td_by_label:
        x_test, y_test = td_by_label[l]
        test_loss, test_acc = model.evaluate(x_test, y_test)
        print('%10s:  %.3f' % (l, test_acc))
        recLblAcc[l] = { 'loss': test_loss, 'accuracy': test_acc }

    if args.modelSaveFile:
        print('')
        model.save(args.modelSaveFile, overwrite=True)
        cfg = json.dumps({
            'train_finish_asctime': time.ctime(t2),
            'train_finish_ts': t2,
            'train_duration_sec': int(t3-t2),
            'label_vector_map': labels_map,
            'SR': SR,
            'SPTGRM_WINDOW_SIZE': SPTGRM_WINDOW_SIZE,
            'SPTGRM_STRIDE': SPTGRM_STRIDE,
            'SPTGRM_SAMPLEN': SPTGRM_SAMPLEN,
            'TRAIN_EPOCHS': TRAIN_EPOCHS,
            'BATCH_SIZE': -1,
            'testset_results': recLblAcc,
        }, indent=2)
        with open(args.modelSaveFile+os.path.sep+'r.cfg.json', 'w') as cw:
            cw.write(cfg)
            cw.write('\n')

        print('model saved')



if __name__ == '__main__':
    main()
